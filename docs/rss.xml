<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Blog Title</title><link>https://purcolin.github.io</link><description>Blog description</description><copyright>Blog Title</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>https://purcolin.github.io</link></image><lastBuildDate>Tue, 13 Aug 2024 09:43:21 +0000</lastBuildDate><managingEditor>Blog Title</managingEditor><ttl>60</ttl><webMaster>Blog Title</webMaster><item><title>重识深度学习：pytorch模型训练</title><link>https://purcolin.github.io/post/zhong-shi-shen-du-xue-xi-%EF%BC%9Apytorch-mo-xing-xun-lian.html</link><description>&#13;
# 数据准备&#13;
&#13;
pytorch能够通过`DataLoader`去迭代输出`Dataset`中的每条batch数据，解耦代码提高可读性与更好的模块化。</description><guid isPermaLink="true">https://purcolin.github.io/post/zhong-shi-shen-du-xue-xi-%EF%BC%9Apytorch-mo-xing-xun-lian.html</guid><pubDate>Tue, 13 Aug 2024 09:42:58 +0000</pubDate></item><item><title>重识深度学习：BERT</title><link>https://purcolin.github.io/post/zhong-shi-shen-du-xue-xi-%EF%BC%9ABERT.html</link><description>## *本文为bert相关实操知识，模型结构等放到tranformer里讲。</description><guid isPermaLink="true">https://purcolin.github.io/post/zhong-shi-shen-du-xue-xi-%EF%BC%9ABERT.html</guid><pubDate>Thu, 08 Aug 2024 08:38:32 +0000</pubDate></item><item><title>重识深度学习：正则化</title><link>https://purcolin.github.io/post/zhong-shi-shen-du-xue-xi-%EF%BC%9A-zheng-ze-hua.html</link><description>一、什么是正则化？&#13;
---------&#13;
&#13;
**正则化**是指在机器学习和统计建模中的一种技术，用于控制模型的复杂度，**防止模型在训练数据上过度拟合（overfitting）**。</description><guid isPermaLink="true">https://purcolin.github.io/post/zhong-shi-shen-du-xue-xi-%EF%BC%9A-zheng-ze-hua.html</guid><pubDate>Thu, 08 Aug 2024 06:45:40 +0000</pubDate></item><item><title>重识深度学习：torch.nn常用类</title><link>https://purcolin.github.io/post/zhong-shi-shen-du-xue-xi-%EF%BC%9Atorch.nn-chang-yong-lei.html</link><description>## Dropout&#13;
```&#13;
torch.nn.Dropout(p=0.5, inplace=False)&#13;
```&#13;
&#13;
功能：Dropout是一种**正则化**手段，能够减少神经元对上层神经元的依赖，一般放在fc层后防止过拟合、提高模型的泛化能力。</description><guid isPermaLink="true">https://purcolin.github.io/post/zhong-shi-shen-du-xue-xi-%EF%BC%9Atorch.nn-chang-yong-lei.html</guid><pubDate>Wed, 07 Aug 2024 02:31:26 +0000</pubDate></item><item><title>重识深度学习：激活函数与全连接</title><link>https://purcolin.github.io/post/zhong-shi-shen-du-xue-xi-%EF%BC%9A-ji-huo-han-shu-yu-quan-lian-jie.html</link><description># 激活函数&#13;
## 1. ReLU&#13;
```&#13;
torch.nn.ReLU(inplace=False)&#13;
```&#13;
$\text{ReLU}(x) = (x)^+ = \max(0, x)$&#13;
![alt text](https://pytorch.org/docs/stable/_images/ReLU.png)&#13;
- inplace (bool) – can optionally do the operation in-place. Default: False&#13;
&#13;
## 2. SoftMax&#13;
```&#13;
torch.nn.Softmax(dim=None)&#13;
```&#13;
$\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}$&#13;
&#13;
使某个维度下的所有值缩放到（0，1）且总和为1.&#13;
## 3. Tanh&#13;
```&#13;
torch.nn.Tanh(*args, **kwargs)&#13;
```&#13;
${Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)}$&#13;
![tanh](https://pytorch.org/docs/stable/_images/Tanh.png)&#13;
## 4. Sigmoid&#13;
```&#13;
torch.nn.Sigmoid(*args, **kwargs)&#13;
```&#13;
${Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}$&#13;
![Sigmoid](https://pytorch.org/docs/stable/_images/Sigmoid.png)&#13;
&#13;
# 全连接&#13;
```&#13;
torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)&#13;
```&#13;
&#13;
- in_features (int) – size of each input sample&#13;
&#13;
- out_features (int) – size of each output sample&#13;
&#13;
- bias (bool) – If set to False, the layer will not learn an additive bias. Default: True&#13;
&#13;
### 输入输出&#13;
&#13;
- $Input Shape = (*,H_{in})$ 其中 $H_{in}$ = in_features &#13;
- $Output Shape = (*,H_{out})$ 其中 $H_{out}$ = out_features &#13;
&#13;
。</description><guid isPermaLink="true">https://purcolin.github.io/post/zhong-shi-shen-du-xue-xi-%EF%BC%9A-ji-huo-han-shu-yu-quan-lian-jie.html</guid><pubDate>Tue, 06 Aug 2024 06:37:58 +0000</pubDate></item><item><title>检索算法#1-排序学习</title><link>https://purcolin.github.io/post/jian-suo-suan-fa-%231--pai-xu-xue-xi.html</link><description>### 训练方法&#13;
- 单文档方法（PointWise Approach）：单文档方法的处理对象是单独的一篇文档，打分后判断是否符合&#13;
- 文档对方法（PairWise Approach)：将排序问题转为了文档对二者前后顺序的判断&#13;
- 文档列表方法（ListWise Approach)：多文档集合训练，打分后排序&#13;
。</description><guid isPermaLink="true">https://purcolin.github.io/post/jian-suo-suan-fa-%231--pai-xu-xue-xi.html</guid><pubDate>Tue, 06 Aug 2024 02:47:19 +0000</pubDate></item><item><title>检索相关术语学习</title><link>https://purcolin.github.io/post/jian-suo-xiang-guan-shu-yu-xue-xi.html</link><description>CTR：点击率&#13;
SPAM：垃圾邮件。</description><guid isPermaLink="true">https://purcolin.github.io/post/jian-suo-xiang-guan-shu-yu-xue-xi.html</guid><pubDate>Tue, 06 Aug 2024 02:45:14 +0000</pubDate></item><item><title>推广搜学习材料</title><link>https://purcolin.github.io/post/tui-guang-sou-xue-xi-cai-liao.html</link><description>&#13;
[搜索基础](https://github.com/wangshusen/SearchEngine/blob/main/Slides/01_Basics_01.pdf)&#13;
&#13;
[模型架构与相关技术](https://github.com/BinFuPKU/CTRRecommenderModels?tab=readme-ov-file)。</description><guid isPermaLink="true">https://purcolin.github.io/post/tui-guang-sou-xue-xi-cai-liao.html</guid><pubDate>Tue, 06 Aug 2024 02:44:38 +0000</pubDate></item><item><title>重识深度学习：CNN</title><link>https://purcolin.github.io/post/zhong-shi-shen-du-xue-xi-%EF%BC%9ACNN.html</link><description># 一、pytorch实现&#13;
CNN的基础架构：&#13;
![CNN基本结构](https://i-blog.csdnimg.cn/blog_migrate/dd24ffc1b67ac2aa6553ded74168bc47.png)&#13;
其中主要的是卷积层与池化层的实现。</description><guid isPermaLink="true">https://purcolin.github.io/post/zhong-shi-shen-du-xue-xi-%EF%BC%9ACNN.html</guid><pubDate>Tue, 06 Aug 2024 02:42:39 +0000</pubDate></item><item><title>this is a test</title><link>https://purcolin.github.io/post/this%20is%20a%20test.html</link><description>test&#13;
# test&#13;
## test_title&#13;
[b]test_bold[/b]。</description><guid isPermaLink="true">https://purcolin.github.io/post/this%20is%20a%20test.html</guid><pubDate>Tue, 06 Aug 2024 02:23:17 +0000</pubDate></item></channel></rss>