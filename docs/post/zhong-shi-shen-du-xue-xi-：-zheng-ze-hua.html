<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    
    <link rel="icon" href="https://github.githubassets.com/favicons/favicon.svg"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="一、什么是正则化？
---------

**正则化**是指在机器学习和统计建模中的一种技术，用于控制模型的复杂度，**防止模型在训练数据上过度拟合（overfitting）**。">
<meta property="og:title" content="重识深度学习：正则化">
<meta property="og:description" content="一、什么是正则化？
---------

**正则化**是指在机器学习和统计建模中的一种技术，用于控制模型的复杂度，**防止模型在训练数据上过度拟合（overfitting）**。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://purcolin.github.io/post/zhong-shi-shen-du-xue-xi-%EF%BC%9A-zheng-ze-hua.html">
<meta property="og:image" content="https://github.githubassets.com/favicons/favicon.svg">
<title>重识深度学习：正则化</title>



</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}
</style>




<body>
    <div id="header">
<h1 class="postTitle">重识深度学习：正则化</h1>
<div class="title-right">
    <a href="https://purcolin.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/purcolin/purcolin.github.io/issues/8" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><h2>一、什么是正则化？</h2>
<p><strong>正则化</strong>是指在机器学习和统计建模中的一种技术，用于控制模型的复杂度，<strong>防止模型在训练数据上过度拟合（overfitting）</strong>。当模型过度拟合时，它会学习到训练数据中的噪声和细微变化，导致在新数据上的性能下降。</p>
<p>正则化通过在模型的<strong>损失函数中引入额外的惩罚项</strong>，来对模型的参数进行约束，从而<strong>降低模型的复杂度</strong>。这个额外的惩罚通常与模型参数的大小或者数量相关，旨在鼓励模型学习简单的规律，而不是过度拟合训练数据。</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c5fffe84b62e1515fd72b8dd478719db4d2a28be5561712647811f9f50291bb5/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f626c6f675f6d6967726174652f37376533363030633566313230343163343265376138306230343138313161312e706e67"><img src="https://camo.githubusercontent.com/c5fffe84b62e1515fd72b8dd478719db4d2a28be5561712647811f9f50291bb5/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f626c6f675f6d6967726174652f37376533363030633566313230343163343265376138306230343138313161312e706e67" alt="" data-canonical-src="https://i-blog.csdnimg.cn/blog_migrate/77e3600c5f12041c42e7a80b041811a1.png" style="max-width: 100%;"></a></p>
<p>在深度学习中，正则化通常涉及到对网络的权重进行约束，以防止它们变得过大或过复杂。最常见的正则化技术之一是 L1 和 L2 正则化，分别通过<strong>对权重的 L1 范数和 L2 范数进行惩罚来实现</strong>。这些技术有助于降低模型的复杂度，并提高模型在未见过的数据上的泛化能力。</p>
<h2>二、正则化的作用？</h2>
<blockquote>
<ol>
<li>
<p><strong>防止过拟合</strong>：正则化通过对模型的复杂度进行限制，防止模型在训练数据上过度拟合。过拟合指的是模型在训练数据上表现良好，但在未见过的数据上表现较差的情况，这可能是因为模型学习到了训练数据中的噪声或者细节，而无法泛化到新数据上。正则化有助于使模型更加简单，从而提高其在未见过的数据上的泛化能力。</p>
</li>
<li>
<p><strong>提高模型的泛化能力</strong>：正则化约束了模型的复杂度，使其更容易泛化到未见过的数据上。通过控制模型的参数大小或数量，正则化可以使模型更加稳定，减少对训练数据的过度依赖，从而提高模型的泛化能力。</p>
</li>
<li>
<p><strong>减少模型的复杂度</strong>：正则化技术通过对模型的参数进行惩罚，促使模型更趋向于简单的解。例如，L1 和 L2 正则化会约束模型的权重，使其趋向于稀疏或较小的值，从而减少模型的复杂度。</p>
</li>
<li>
<p><strong>控制模型的学习速度</strong>：正则化技术可以对模型的学习速度进行调节，防止模型在训练过程中权重变化过大，从而导致优化过程不稳定。这有助于加速模型的收敛，并提高模型在训练数据上的表现。</p>
</li>
<li>
<p><strong>提高模型的鲁棒性</strong>：正则化有助于使模型更加鲁棒，即对输入数据的微小变化不敏感。通过降低模型的复杂度，正则化可以减少模型对训练数据中噪声的敏感度，从而提高模型的鲁棒性。</p>
</li>
</ol>
</blockquote>
<h2>三、常见的正则化方法</h2>
<blockquote>
<ol>
<li>
<p><strong>L1 正则化</strong>：也称为 Lasso 正则化，它通过在模型的损失函数中增加权重的 L1 范数（权重向量的绝对值之和）来实现正则化。L1 正则化倾向于产生稀疏权重矩阵，即将一些权重推向零，从而实现特征选择的效果。</p>
</li>
<li>
<p><strong>L2 正则化</strong>：也称为 Ridge 正则化，它通过在模型的损失函数中增加权重的 L2 范数（权重向量的平方和）来实现正则化。L2 正则化会使权重值变得较小，但不会直接导致权重稀疏，因此不具有特征选择的作用，但可以有效地控制模型的复杂度。</p>
</li>
<li>
<p><strong>Elastic Net 正则化</strong>：Elastic Net 是 L1 和 L2 正则化的组合，它在损失函数中同时使用 L1 和 L2 范数，可以综合两者的优点。</p>
</li>
<li>
<p><strong>Dropout</strong>：Dropout 是一种特殊的正则化技术，通过在训练过程中随机地丢弃（将其权重置为零）网络中的部分神经元，以及它们的连接，来减少神经网络的复杂度。这样可以防止神经元之间的共适应性，从而减少过拟合。</p>
</li>
<li>
<p><strong>早停（Early Stopping）</strong>：早停是一种简单而有效的正则化方法，它在训练过程中监视模型在验证集上的性能，一旦验证集上的性能开始下降，就停止训练。这样可以避免模型在训练集上过拟合。</p>
</li>
<li>
<p><strong>数据增强（Data Augmentation）</strong>：数据增强是通过对训练数据进行变换来增加数据的多样性，从而减少过拟合的风险。例如，在图像分类任务中可以进行随机裁剪、旋转、翻转等操作来增加训练数据的数量和多样性。</p>
</li>
<li>
<p><strong>批量归一化（Batch Normalization）</strong>：批量归一化是一种通过对每个批次的输入进行归一化来加速训练并减少过拟合的技术。它可以使得每一层的输入分布稳定，从而更容易优化模型。</p>
</li>
<li>
<p><strong>权重衰减（Weight Decay）</strong>：权重衰减是一种通过在损失函数中增加权重的平方和或绝对值之和来实现正则化的技术。它等价于对权重参数进行 L2 正则化。</p>
</li>
</ol>
</blockquote>
<h2>四、详解L1正则化 </h2>
<p>L1 正则化，也称为 Lasso 正则化，是一种常用的正则化技术，用于控制模型的复杂度和防止过拟合。它的原理是通过在模型的损失函数中增加权重的 L1 范数（权重向量的绝对值之和）作为惩罚项，从而鼓励模型产生稀疏权重，即让一部分权重趋近于零，实现特征选择的效果。</p>
<p><strong>L1 正则化的损失函数：</strong></p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2159baa09bb6d71bc674ecbd5db3a48173f57cebf9f0e4528dd3e1f016e2b34a/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c5f253742253543746578742537424c312537442537442532302533442532304c5f25374225354374657874253742646174612537442537442532302b2532302535436c616d62646125323025354373756d5f25374269253344312537442535452537426e253744253230253743775f69253743"><img src="https://camo.githubusercontent.com/2159baa09bb6d71bc674ecbd5db3a48173f57cebf9f0e4528dd3e1f016e2b34a/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c5f253742253543746578742537424c312537442537442532302533442532304c5f25374225354374657874253742646174612537442537442532302b2532302535436c616d62646125323025354373756d5f25374269253344312537442535452537426e253744253230253743775f69253743" alt="L_{\text{L1}} = L_{\text{data}} + \lambda \sum_{i=1}^{n} |w_i|" data-canonical-src="https://latex.csdn.net/eq?L_%7B%5Ctext%7BL1%7D%7D%20%3D%20L_%7B%5Ctext%7Bdata%7D%7D%20+%20%5Clambda%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%7Cw_i%7C" style="max-width: 100%;"></a></p>
<blockquote>
<p>其中：<br>
- <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/63a2ab5ba129ba32850cff2cb2347acfb99f05e5b83fc2bf20a43be4eae4e3a0/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c5f2537422535437465787425374264617461253744253744"><img src="https://camo.githubusercontent.com/63a2ab5ba129ba32850cff2cb2347acfb99f05e5b83fc2bf20a43be4eae4e3a0/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c5f2537422535437465787425374264617461253744253744" alt="L_{\text{data}}" data-canonical-src="https://latex.csdn.net/eq?L_%7B%5Ctext%7Bdata%7D%7D" style="max-width: 100%;"></a>是模型的数据损失，通常是模型的预测值与真实标签之间的误差，如均方误差（MSE）或交叉熵损失（Cross-entropy loss）。<br>
- <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a69e315e1999391b4f1e1b6f711e70b691cb7cb73d76139f5397d6693d474c88/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436c616d626461"><img src="https://camo.githubusercontent.com/a69e315e1999391b4f1e1b6f711e70b691cb7cb73d76139f5397d6693d474c88/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436c616d626461" alt="\lambda" data-canonical-src="https://latex.csdn.net/eq?%5Clambda" style="max-width: 100%;"></a>是正则化参数，用于控制正则化项的强度。<br>
- <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/87dae7905f197b3c67ca057302e48a2d655edaa7ae54d9a986dc64f8bd9c4ecf/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f253743775f69253743"><img src="https://camo.githubusercontent.com/87dae7905f197b3c67ca057302e48a2d655edaa7ae54d9a986dc64f8bd9c4ecf/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f253743775f69253743" alt="|w_i|" data-canonical-src="https://latex.csdn.net/eq?%7Cw_i%7C" style="max-width: 100%;"></a> 表示模型的权重的绝对值。</p>
</blockquote>
<p><strong>公式推导：</strong> </p>
<p><strong>L1 正则化</strong>是一种通过在模型的损失函数中增加权重的 <strong>L1 范数作为惩罚项</strong>来控制模型复杂度的技术。L1 范数是向量中各个元素的绝对值之和，其数学表示如下：</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d528ba2d004e2f4584f3ac9563a44040ed346ba658b95d6b8b0c10eaa113850d/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2537432537432535436d6174686266253742772537442537432537435f3125323025334425323025354373756d5f25374269253344312537442535452537426e253744253230253743775f69253743"><img src="https://camo.githubusercontent.com/d528ba2d004e2f4584f3ac9563a44040ed346ba658b95d6b8b0c10eaa113850d/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2537432537432535436d6174686266253742772537442537432537435f3125323025334425323025354373756d5f25374269253344312537442535452537426e253744253230253743775f69253743" alt="||\mathbf{w}||1 = \sum{i=1}^{n} |w_i|" data-canonical-src="https://latex.csdn.net/eq?%7C%7C%5Cmathbf%7Bw%7D%7C%7C_1%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%7Cw_i%7C" style="max-width: 100%;"></a></p>
<p>其中 <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/79656e66eddad60b36d5856c55b587faa3b26c093e75a75a4d1561eb66bbee4c/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d617468626625374277253744"><img src="https://camo.githubusercontent.com/79656e66eddad60b36d5856c55b587faa3b26c093e75a75a4d1561eb66bbee4c/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d617468626625374277253744" alt="\mathbf{w}" data-canonical-src="https://latex.csdn.net/eq?%5Cmathbf%7Bw%7D" style="max-width: 100%;"></a>是模型的权重向量，<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/71758095f0db5326fda8bc956402d83e6af35e814c48b531ce240c1303adbfef/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f6e"><img src="https://camo.githubusercontent.com/71758095f0db5326fda8bc956402d83e6af35e814c48b531ce240c1303adbfef/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f6e" alt="n" data-canonical-src="https://latex.csdn.net/eq?n" style="max-width: 100%;"></a>是权重向量的长度，即权重的数量。</p>
<p>在 L1 正则化中，惩罚项可以写为权重的 L1 范数：</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c09afc96654e02f383905902de5e44b3a71725ba782335e2ff251e1f92ba050e/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535437465787425374270656e616c74792537442532302533442532302535436c616d6264612532302537432537432535436d6174686266253742772537442537432537435f31"><img src="https://camo.githubusercontent.com/c09afc96654e02f383905902de5e44b3a71725ba782335e2ff251e1f92ba050e/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535437465787425374270656e616c74792537442532302533442532302535436c616d6264612532302537432537432535436d6174686266253742772537442537432537435f31" alt="\text{penalty} = \lambda ||\mathbf{w}||_1" data-canonical-src="https://latex.csdn.net/eq?%5Ctext%7Bpenalty%7D%20%3D%20%5Clambda%20%7C%7C%5Cmathbf%7Bw%7D%7C%7C_1" style="max-width: 100%;"></a></p>
<p>其中 <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a69e315e1999391b4f1e1b6f711e70b691cb7cb73d76139f5397d6693d474c88/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436c616d626461"><img src="https://camo.githubusercontent.com/a69e315e1999391b4f1e1b6f711e70b691cb7cb73d76139f5397d6693d474c88/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436c616d626461" alt="\lambda" data-canonical-src="https://latex.csdn.net/eq?%5Clambda" style="max-width: 100%;"></a>是正则化参数，用于控制正则化的强度。</p>
<p>现在，我们来推导一下 L1 正则化的损失函数。假设我们有一个带有 L1 正则化的线性回归模型，其损失函数可以表示为：</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/16380ea3736386eb581acc1aca543bcf2aebac8093bca29665926b0c5d7e1d42/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c2532382535436d6174686266253742772537442532392532302533442532304c5f25374225354374657874253742646174612537442537442532382535436d6174686266253742772537442532392532302b2532302535436c616d6264612532302537432537432535436d6174686266253742772537442537432537435f31"><img src="https://camo.githubusercontent.com/16380ea3736386eb581acc1aca543bcf2aebac8093bca29665926b0c5d7e1d42/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c2532382535436d6174686266253742772537442532392532302533442532304c5f25374225354374657874253742646174612537442537442532382535436d6174686266253742772537442532392532302b2532302535436c616d6264612532302537432537432535436d6174686266253742772537442537432537435f31" alt="L(\mathbf{w}) = L_{\text{data}}(\mathbf{w}) + \lambda ||\mathbf{w}||_1" data-canonical-src="https://latex.csdn.net/eq?L%28%5Cmathbf%7Bw%7D%29%20%3D%20L_%7B%5Ctext%7Bdata%7D%7D%28%5Cmathbf%7Bw%7D%29%20+%20%5Clambda%20%7C%7C%5Cmathbf%7Bw%7D%7C%7C_1" style="max-width: 100%;"></a></p>
<p>其中 <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e7cb61d57199e5452a64ff201fa347aac3ca6539bd95f8016657d070b82e360d/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c5f25374225354374657874253742646174612537442537442532382535436d617468626625374277253744253239"><img src="https://camo.githubusercontent.com/e7cb61d57199e5452a64ff201fa347aac3ca6539bd95f8016657d070b82e360d/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c5f25374225354374657874253742646174612537442537442532382535436d617468626625374277253744253239" alt="L_{\text{data}}(\mathbf{w})" data-canonical-src="https://latex.csdn.net/eq?L_%7B%5Ctext%7Bdata%7D%7D%28%5Cmathbf%7Bw%7D%29" style="max-width: 100%;"></a>是模型的数据损失，通常是模型的预测值与真实标签之间的误差。</p>
<p>我们的<strong>目标是最小化整个损失函数</strong>。为了找到最小化损失函数的权重 <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/79656e66eddad60b36d5856c55b587faa3b26c093e75a75a4d1561eb66bbee4c/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d617468626625374277253744"><img src="https://camo.githubusercontent.com/79656e66eddad60b36d5856c55b587faa3b26c093e75a75a4d1561eb66bbee4c/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d617468626625374277253744" alt="\mathbf{w}" data-canonical-src="https://latex.csdn.net/eq?%5Cmathbf%7Bw%7D" style="max-width: 100%;"></a>，我们可以使用梯度下降等优化算法。在梯度下降中，我们需要计算损失函数关于权重的梯度，然后根据梯度的方向和大小来更新权重。</p>
<p>现在，我们来推导损失函数关于权重的梯度。为了简化推导，我们假设 <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e7cb61d57199e5452a64ff201fa347aac3ca6539bd95f8016657d070b82e360d/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c5f25374225354374657874253742646174612537442537442532382535436d617468626625374277253744253239"><img src="https://camo.githubusercontent.com/e7cb61d57199e5452a64ff201fa347aac3ca6539bd95f8016657d070b82e360d/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c5f25374225354374657874253742646174612537442537442532382535436d617468626625374277253744253239" alt="L_{\text{data}}(\mathbf{w})" data-canonical-src="https://latex.csdn.net/eq?L_%7B%5Ctext%7Bdata%7D%7D%28%5Cmathbf%7Bw%7D%29" style="max-width: 100%;"></a>是均方误差损失函数，即：</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2b9231c4a7ed4f234579fe73a3c0da363a63c2aa5dcc99fa0e199bba09705056/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c5f25374225354374657874253742646174612537442537442532382535436d6174686266253742772537442532392532302533442532302535436672616325374231253744253742322537442532302537432537432535436d6174686266253742792537442532302d2532302535436d6174686266253742582537442535436d6174686266253742772537442537432537435f3225354532"><img src="https://camo.githubusercontent.com/2b9231c4a7ed4f234579fe73a3c0da363a63c2aa5dcc99fa0e199bba09705056/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c5f25374225354374657874253742646174612537442537442532382535436d6174686266253742772537442532392532302533442532302535436672616325374231253744253742322537442532302537432537432535436d6174686266253742792537442532302d2532302535436d6174686266253742582537442535436d6174686266253742772537442537432537435f3225354532" alt="L_{\text{data}}(\mathbf{w}) = \frac{1}{2} ||\mathbf{y} - \mathbf{X}\mathbf{w}||_2^2" data-canonical-src="https://latex.csdn.net/eq?L_%7B%5Ctext%7Bdata%7D%7D%28%5Cmathbf%7Bw%7D%29%20%3D%20%5Cfrac%7B1%7D%7B2%7D%20%7C%7C%5Cmathbf%7By%7D%20-%20%5Cmathbf%7BX%7D%5Cmathbf%7Bw%7D%7C%7C_2%5E2" style="max-width: 100%;"></a></p>
<p>其中 <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9c4d215bc444eb1c7e24366e3b2c4dfaaaa704b8c31e108f078e666294a52bc6/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d617468626625374258253744"><img src="https://camo.githubusercontent.com/9c4d215bc444eb1c7e24366e3b2c4dfaaaa704b8c31e108f078e666294a52bc6/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d617468626625374258253744" alt="\mathbf{X}" data-canonical-src="https://latex.csdn.net/eq?%5Cmathbf%7BX%7D" style="max-width: 100%;"></a>是输入特征矩阵，<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/846ff0a44dddae753665b31982546393572e380768c11f480dae34aa87ce55d8/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d617468626625374279253744"><img src="https://camo.githubusercontent.com/846ff0a44dddae753665b31982546393572e380768c11f480dae34aa87ce55d8/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d617468626625374279253744" alt="\mathbf{y}" data-canonical-src="https://latex.csdn.net/eq?%5Cmathbf%7By%7D" style="max-width: 100%;"></a>是真实标签向量。</p>
<p>我们的目标是最小化总损失函数：</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/85d1169c986971366b808f70ba214cf39ae08568a968a6edfef6972882f574ae/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c2532382535436d6174686266253742772537442532392532302533442532302535436672616325374231253744253742322537442532302537432537432535436d6174686266253742792537442532302d2532302535436d6174686266253742582537442535436d6174686266253742772537442537432537435f32253545322532302b2532302535436c616d6264612532302537432537432535436d6174686266253742772537442537432537435f31"><img src="https://camo.githubusercontent.com/85d1169c986971366b808f70ba214cf39ae08568a968a6edfef6972882f574ae/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c2532382535436d6174686266253742772537442532392532302533442532302535436672616325374231253744253742322537442532302537432537432535436d6174686266253742792537442532302d2532302535436d6174686266253742582537442535436d6174686266253742772537442537432537435f32253545322532302b2532302535436c616d6264612532302537432537432535436d6174686266253742772537442537432537435f31" alt="L(\mathbf{w}) = \frac{1}{2} ||\mathbf{y} - \mathbf{X}\mathbf{w}||_2^2 + \lambda ||\mathbf{w}||_1" data-canonical-src="https://latex.csdn.net/eq?L%28%5Cmathbf%7Bw%7D%29%20%3D%20%5Cfrac%7B1%7D%7B2%7D%20%7C%7C%5Cmathbf%7By%7D%20-%20%5Cmathbf%7BX%7D%5Cmathbf%7Bw%7D%7C%7C_2%5E2%20+%20%5Clambda%20%7C%7C%5Cmathbf%7Bw%7D%7C%7C_1" style="max-width: 100%;"></a></p>
<p>现在，我们对<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/986888cc74b4096e0894805492442dc40297c0e6e042ce41ce07d57cff44f3e3/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c2532382535436d617468626625374277253744253239"><img src="https://camo.githubusercontent.com/986888cc74b4096e0894805492442dc40297c0e6e042ce41ce07d57cff44f3e3/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c2532382535436d617468626625374277253744253239" alt="L(\mathbf{w})" data-canonical-src="https://latex.csdn.net/eq?L%28%5Cmathbf%7Bw%7D%29" style="max-width: 100%;"></a>求导数，得到梯度：</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c5e1c32604f5712588bdcbf15586c0700455eb41872d026b29a2ad543eff2268/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436e61626c612532304c2532382535436d6174686266253742772537442532392532302533442532302d2535436d617468626625374258253744253545542532302532382535436d6174686266253742792537442532302d2532302535436d6174686266253742582537442535436d6174686266253742772537442532392532302b2532302535436c616d626461253230253543746578742537427369676e2537442532382535436d617468626625374277253744253239"><img src="https://camo.githubusercontent.com/c5e1c32604f5712588bdcbf15586c0700455eb41872d026b29a2ad543eff2268/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436e61626c612532304c2532382535436d6174686266253742772537442532392532302533442532302d2535436d617468626625374258253744253545542532302532382535436d6174686266253742792537442532302d2532302535436d6174686266253742582537442535436d6174686266253742772537442532392532302b2532302535436c616d626461253230253543746578742537427369676e2537442532382535436d617468626625374277253744253239" alt="\nabla L(\mathbf{w}) = -\mathbf{X}^T (\mathbf{y} - \mathbf{X}\mathbf{w}) + \lambda \text{sign}(\mathbf{w})" data-canonical-src="https://latex.csdn.net/eq?%5Cnabla%20L%28%5Cmathbf%7Bw%7D%29%20%3D%20-%5Cmathbf%7BX%7D%5ET%20%28%5Cmathbf%7By%7D%20-%20%5Cmathbf%7BX%7D%5Cmathbf%7Bw%7D%29%20+%20%5Clambda%20%5Ctext%7Bsign%7D%28%5Cmathbf%7Bw%7D%29" style="max-width: 100%;"></a></p>
<p>其中 <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a2a87141ea046f9d868d78d9857103eb27e4f3ecdac54855a90c5c2d5432dcf9/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f253543746578742537427369676e2537442532382535436d617468626625374277253744253239"><img src="https://camo.githubusercontent.com/a2a87141ea046f9d868d78d9857103eb27e4f3ecdac54855a90c5c2d5432dcf9/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f253543746578742537427369676e2537442532382535436d617468626625374277253744253239" alt="\text{sign}(\mathbf{w})" data-canonical-src="https://latex.csdn.net/eq?%5Ctext%7Bsign%7D%28%5Cmathbf%7Bw%7D%29" style="max-width: 100%;"></a>是权重向量 <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/79656e66eddad60b36d5856c55b587faa3b26c093e75a75a4d1561eb66bbee4c/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d617468626625374277253744"><img src="https://camo.githubusercontent.com/79656e66eddad60b36d5856c55b587faa3b26c093e75a75a4d1561eb66bbee4c/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d617468626625374277253744" alt="\mathbf{w}" data-canonical-src="https://latex.csdn.net/eq?%5Cmathbf%7Bw%7D" style="max-width: 100%;"></a>各个元素的符号函数。这意味着每个权重的梯度由数据损失和正则化项的梯度之和组成。</p>
<p>最后，我们可以使用梯度下降等优化算法来最小化损失函数，并找到最优的权重 <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/79656e66eddad60b36d5856c55b587faa3b26c093e75a75a4d1561eb66bbee4c/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d617468626625374277253744"><img src="https://camo.githubusercontent.com/79656e66eddad60b36d5856c55b587faa3b26c093e75a75a4d1561eb66bbee4c/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d617468626625374277253744" alt="\mathbf{w}" data-canonical-src="https://latex.csdn.net/eq?%5Cmathbf%7Bw%7D" style="max-width: 100%;"></a>。在优化过程中，L1 正则化项会促使一些权重趋向于零，从而实现特征选择的效果，降低模型的复杂度，防止</p>
<p><strong>可视化对比L1正则化效果：</strong> </p>
<p>过拟合Python 代码，用于生成带有噪声的线性数据集，并分别应用没有 L1 正则化和有 L1 正则化的线性模型来拟合数据，并在同一页面可视化对比两种情况的结果：</p>
<pre class="notranslate"><code class="notranslate">import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, Lasso

# 生成带有噪声的线性数据集
np.random.seed(0)
X = np.random.rand(100, 1)  # 特征
y = 3 * X.squeeze() + np.random.normal(0, 0.3, 100)  # 标签

# 不使用正则化的线性回归模型
linear_model = LinearRegression()
linear_model.fit(X, y)

# 使用 L1 正则化的 Lasso 回归模型
lasso_model = Lasso(alpha=0.2)  # 正则化参数 alpha
lasso_model.fit(X, y)

# 可视化结果
plt.figure(figsize=(12, 6))

# 绘制原始数据和线性回归模型拟合结果
plt.subplot(1, 2, 1)
plt.scatter(X, y, color='blue', label='Data')
plt.plot(X, linear_model.predict(X), color='red', linewidth=2, label='Linear Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Without L1 Regularization')
plt.legend()

# 绘制原始数据和 Lasso 回归模型拟合结果
plt.subplot(1, 2, 2)
plt.scatter(X, y, color='blue', label='Data')
plt.plot(X, lasso_model.predict(X), color='green', linewidth=2, label='Lasso Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.title('With L1 Regularization (Lasso)')
plt.legend()

plt.show()

</code></pre>
<p>结果展示包含两个子图的图像，左侧子图展示了没有应用 L1 正则化的线性回归模型拟合结果，右侧子图展示了应用了 L1 正则化的 Lasso 回归模型拟合结果。通过这两个子图的对比，我们可以清晰地看到 L1 正则化的作用，它使得模型的权重变得更加稀疏，从而实现了特征选择的效果。</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a2cecd8b48e44a81ad43af5317a620fe3e9f2bf3ee0d350ff044186610898dc1/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f626c6f675f6d6967726174652f38643161616264313765376363313864653338303336386165373363363665622e706e67"><img src="https://camo.githubusercontent.com/a2cecd8b48e44a81ad43af5317a620fe3e9f2bf3ee0d350ff044186610898dc1/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f626c6f675f6d6967726174652f38643161616264313765376363313864653338303336386165373363363665622e706e67" alt="" data-canonical-src="https://i-blog.csdnimg.cn/blog_migrate/8d1aabd17e7cc18de380368ae73c66eb.png" style="max-width: 100%;"></a></p>
<h2> 五、详解L2正则化</h2>
<p><strong>L2 正则化，也称为 Ridge 正则化。它通过向模型的损失函数添加一个权重参数的 L2 范数的惩罚项</strong>来实现。下面我们来详细解释一下 L2 正则化的原理和数学公式。</p>
<p><strong>数学公式：</strong></p>
<p>在 L2 正则化中，惩罚项通常被定义为权重参数的 L2 范数的平方。具体地，L2 正则化的损失函数可以表示为：</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b8881a5f9c523c3127eee5a525b96060f2807742e19add2dae661bf9caf8ec92/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c5f253742253543746578742537424c322537442537442532302533442532304c5f25374225354374657874253742646174612537442537442532302b2532302535436c616d6264612532302537432537432535436d6174686266253742772537442537432537435f3225354532"><img src="https://camo.githubusercontent.com/b8881a5f9c523c3127eee5a525b96060f2807742e19add2dae661bf9caf8ec92/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c5f253742253543746578742537424c322537442537442532302533442532304c5f25374225354374657874253742646174612537442537442532302b2532302535436c616d6264612532302537432537432535436d6174686266253742772537442537432537435f3225354532" alt="L_{\text{L2}} = L_{\text{data}} + \lambda ||\mathbf{w}||_2^2" data-canonical-src="https://latex.csdn.net/eq?L_%7B%5Ctext%7BL2%7D%7D%20%3D%20L_%7B%5Ctext%7Bdata%7D%7D%20+%20%5Clambda%20%7C%7C%5Cmathbf%7Bw%7D%7C%7C_2%5E2" style="max-width: 100%;"></a></p>
<blockquote>
<p>其中：<br>
- <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/63a2ab5ba129ba32850cff2cb2347acfb99f05e5b83fc2bf20a43be4eae4e3a0/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c5f2537422535437465787425374264617461253744253744"><img src="https://camo.githubusercontent.com/63a2ab5ba129ba32850cff2cb2347acfb99f05e5b83fc2bf20a43be4eae4e3a0/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c5f2537422535437465787425374264617461253744253744" alt="L_{\text{data}}" data-canonical-src="https://latex.csdn.net/eq?L_%7B%5Ctext%7Bdata%7D%7D" style="max-width: 100%;"></a>是模型的数据损失，通常是模型的预测值与真实标签之间的误差。<br>
- <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a69e315e1999391b4f1e1b6f711e70b691cb7cb73d76139f5397d6693d474c88/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436c616d626461"><img src="https://camo.githubusercontent.com/a69e315e1999391b4f1e1b6f711e70b691cb7cb73d76139f5397d6693d474c88/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436c616d626461" alt="\lambda" data-canonical-src="https://latex.csdn.net/eq?%5Clambda" style="max-width: 100%;"></a>是正则化参数，用于控制正则化的强度。<br>
- <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/860d2bfc066859ab79920ba45351293e195fc9c9f51b14d8fbd71cc4d498f450/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2537432537432535436d6174686266253742772537442537432537435f3225354532"><img src="https://camo.githubusercontent.com/860d2bfc066859ab79920ba45351293e195fc9c9f51b14d8fbd71cc4d498f450/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2537432537432535436d6174686266253742772537442537432537435f3225354532" alt="||\mathbf{w}||_2^2" data-canonical-src="https://latex.csdn.net/eq?%7C%7C%5Cmathbf%7Bw%7D%7C%7C_2%5E2" style="max-width: 100%;"></a>是权重向量<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/79656e66eddad60b36d5856c55b587faa3b26c093e75a75a4d1561eb66bbee4c/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d617468626625374277253744"><img src="https://camo.githubusercontent.com/79656e66eddad60b36d5856c55b587faa3b26c093e75a75a4d1561eb66bbee4c/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d617468626625374277253744" alt="\mathbf{w}" data-canonical-src="https://latex.csdn.net/eq?%5Cmathbf%7Bw%7D" style="max-width: 100%;"></a> 的 L2 范数的平方，表示为权重向量中各个参数的平方和。</p>
<p>使用 L2 正则化的损失函数时，优化算法在优化过程中会同时考虑数据损失和正则化项，从而在保持对训练数据的拟合能力的同时，尽可能减小模型参数的大小，降低模型的复杂度。</p>
</blockquote>
<p><strong>可视化L2正则化效果：</strong> </p>
<p>首先，我们将生成一个带有噪声的线性数据集，并分别使用没有 L2 正则化的普通线性回归模型和带有 L2 正则化的 Ridge 回归模型来拟合数据。</p>
<pre class="notranslate"><code class="notranslate">import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, Ridge

# 生成带有噪声的线性数据集
np.random.seed(0)
X = np.random.rand(100, 1)  # 特征
y = 3 * X.squeeze() + np.random.normal(0, 0.3, 100)  # 标签

# 没有使用 L2 正则化的线性回归模型
linear_model = LinearRegression()
linear_model.fit(X, y)

# 使用 L2 正则化的 Ridge 回归模型
ridge_model = Ridge(alpha=1.0)  # 正则化参数 alpha
ridge_model.fit(X, y)

# 可视化结果
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', label='Data')
plt.plot(X, linear_model.predict(X), color='red', linewidth=2, label='Linear Regression (No L2 Regularization)')
plt.plot(X, ridge_model.predict(X), color='green', linewidth=2, label='Ridge Regression (L2 Regularization)')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Comparison of Linear Regression with and without L2 Regularization')
plt.legend()
plt.show()
</code></pre>
<p> 生成一个散点图，其中蓝色的点表示原始数据，红色的线表示没有 L2 正则化的普通线性回归模型的拟合结果，绿色的线表示带有 L2 正则化的 Ridge 回归模型的拟合结果。通过观察这张图，我们可以直观地比较两种模型的拟合效果，以及 L2 正则化对模型的影响。</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/66259e9e26f4e368488d8c0da391fee9f9b0edd09f1f4e5fba06902c80214601/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f626c6f675f6d6967726174652f30643135303939643364303135636135643264396632623230623330653631342e706e67"><img src="https://camo.githubusercontent.com/66259e9e26f4e368488d8c0da391fee9f9b0edd09f1f4e5fba06902c80214601/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f626c6f675f6d6967726174652f30643135303939643364303135636135643264396632623230623330653631342e706e67" alt="" data-canonical-src="https://i-blog.csdnimg.cn/blog_migrate/0d15099d3d015ca5d2d9f2b20b30e614.png" style="max-width: 100%;"></a></p>
<h2>六、详解Dropout方法</h2>
<p>Dropout 是一种在<strong>神经网络中常用的正则化技术</strong>，用于减少过拟合。其原理是在网络的训练过程中，随机地将部分神经元的输出置为零（即失活），从而使得网络在每次迭代时都在不同的子网络上训练，以减少神经元之间的复杂依赖关系，从而增强模型的泛化能力。</p>
<p><strong>工作原理：</strong></p>
<blockquote>
<ol>
<li>
<p><strong>随机失活神经元</strong>：在每次训练迭代时，Dropout 方法会以一定的概率（通常为 0.5）随机地将某些神经元的输出置为零，即使得这些神经元在此次迭代中不参与前向传播和反向传播。这样可以阻止网络过度依赖于某些特定的神经元，增强模型的泛化能力。</p>
</li>
<li>
<p><strong>训练时与测试时的区别</strong>：在训练时，通过随机失活神经元来减少过拟合；而在测试时，所有的神经元都保持活跃，但是输出值需要按照训练时的概率进行缩放，以保持期望输出的一致性。</p>
</li>
<li>
<p><strong>Dropout的随机性</strong>：Dropout 是通过在每次迭代中随机选择要失活的神经元来实现的。这种随机性会导致网络在每次迭代时都训练在不同的子网络上，从而相当于训练了多个不同的模型，最终取平均或者加权平均作为最终的预测结果。</p>
</li>
</ol>
</blockquote>
<p><strong>Dropout的优点：</strong></p>
<blockquote>
<ul>
<li><strong>减少过拟合</strong>：通过随机失活部分神经元，阻止网络过度拟合训练数据，从而提高了模型的泛化能力。</li>
<li><strong>简单易用</strong>：Dropout 是一种简单而有效的正则化技术，可以直接应用于现有的神经网络模型中，而无需对网络结构进行修改。</li>
</ul>
</blockquote>
<p><strong>数学公式：</strong></p>
<p>在数学上，Dropout 的原理可以通过以下方式进行表述。</p>
<p>假设我们有一个具有<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8d9e9f6cf979873a8a7b6ffa8f40867018501613086697d8ff0b2ff08493881f/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c"><img src="https://camo.githubusercontent.com/8d9e9f6cf979873a8a7b6ffa8f40867018501613086697d8ff0b2ff08493881f/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c" alt="L" data-canonical-src="https://latex.csdn.net/eq?L" style="max-width: 100%;"></a>个隐藏层的神经网络，其中每个隐藏层<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6a4395d82439bacc026584dd128362e93193053929c7633ec045f437c7a85221/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f6c"><img src="https://camo.githubusercontent.com/6a4395d82439bacc026584dd128362e93193053929c7633ec045f437c7a85221/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f6c" alt="l" data-canonical-src="https://latex.csdn.net/eq?l" style="max-width: 100%;"></a>包含 <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/dd5e4aa5addc6fb3592844e5c03016d9292481db986d8f3ac9ae1924b505ab4d/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f6e2535452537422535426c253544253744"><img src="https://camo.githubusercontent.com/dd5e4aa5addc6fb3592844e5c03016d9292481db986d8f3ac9ae1924b505ab4d/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f6e2535452537422535426c253544253744" alt="n^{[l]}" data-canonical-src="https://latex.csdn.net/eq?n%5E%7B%5Bl%5D%7D" style="max-width: 100%;"></a>个神经元。对于每个隐藏层 <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6a4395d82439bacc026584dd128362e93193053929c7633ec045f437c7a85221/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f6c"><img src="https://camo.githubusercontent.com/6a4395d82439bacc026584dd128362e93193053929c7633ec045f437c7a85221/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f6c" alt="l" data-canonical-src="https://latex.csdn.net/eq?l" style="max-width: 100%;"></a>，我们定义一个二进制掩码向量<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ad7e730d3dd6f1d78bf18147a0d369cc856b55f9f1a1145e8aa81c16576635a3/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d6174686266253742642537442535452537422535426c253544253744"><img src="https://camo.githubusercontent.com/ad7e730d3dd6f1d78bf18147a0d369cc856b55f9f1a1145e8aa81c16576635a3/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d6174686266253742642537442535452537422535426c253544253744" alt="\mathbf{d}^{[l]}" data-canonical-src="https://latex.csdn.net/eq?%5Cmathbf%7Bd%7D%5E%7B%5Bl%5D%7D" style="max-width: 100%;"></a>，其中<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/dd389c4a75d1fb1996e5c6b21d14f41761b817e570c892685091b25d4f8182b5/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f645f253742692537442535452537422535426c253544253744"><img src="https://camo.githubusercontent.com/dd389c4a75d1fb1996e5c6b21d14f41761b817e570c892685091b25d4f8182b5/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f645f253742692537442535452537422535426c253544253744" alt="d_{i}^{[l]}" data-canonical-src="https://latex.csdn.net/eq?d_%7Bi%7D%5E%7B%5Bl%5D%7D" style="max-width: 100%;"></a> 表示第<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8de688ded3d33ed2a3a371074b5f4319d97f400e32696e7b12c971027bcfc027/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f69"><img src="https://camo.githubusercontent.com/8de688ded3d33ed2a3a371074b5f4319d97f400e32696e7b12c971027bcfc027/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f69" alt="i" data-canonical-src="https://latex.csdn.net/eq?i" style="max-width: 100%;"></a>个神经元是否被保留（未失活）。</p>
<p>在训练期间，对于每个训练示例<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d7598116fd4ce14b0f320184b9919bc18b26065dbfc359c29738ffc80ce26db4/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f74"><img src="https://camo.githubusercontent.com/d7598116fd4ce14b0f320184b9919bc18b26065dbfc359c29738ffc80ce26db4/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f74" alt="t" data-canonical-src="https://latex.csdn.net/eq?t" style="max-width: 100%;"></a>，Dropout 方法将随机地将掩码向量<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ad7e730d3dd6f1d78bf18147a0d369cc856b55f9f1a1145e8aa81c16576635a3/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d6174686266253742642537442535452537422535426c253544253744"><img src="https://camo.githubusercontent.com/ad7e730d3dd6f1d78bf18147a0d369cc856b55f9f1a1145e8aa81c16576635a3/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d6174686266253742642537442535452537422535426c253544253744" alt="\mathbf{d}^{[l]}" data-canonical-src="https://latex.csdn.net/eq?%5Cmathbf%7Bd%7D%5E%7B%5Bl%5D%7D" style="max-width: 100%;"></a>应用于每个隐藏层<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6a4395d82439bacc026584dd128362e93193053929c7633ec045f437c7a85221/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f6c"><img src="https://camo.githubusercontent.com/6a4395d82439bacc026584dd128362e93193053929c7633ec045f437c7a85221/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f6c" alt="l" data-canonical-src="https://latex.csdn.net/eq?l" style="max-width: 100%;"></a> 的输出，从而产生一个新的损失函数 <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ef8c3174036253026cda121fe03fdd7c558e487dee1571521d2ea2119800fec1/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c2535452537422535426c253544253744"><img src="https://camo.githubusercontent.com/ef8c3174036253026cda121fe03fdd7c558e487dee1571521d2ea2119800fec1/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c2535452537422535426c253544253744" alt="L^{[l]}" data-canonical-src="https://latex.csdn.net/eq?L%5E%7B%5Bl%5D%7D" style="max-width: 100%;"></a>：</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7b9009df0b7ca290c19660274a0f69ddb0d4cb7bb833ed763f6d7318ffbf1175/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c2535452537422535426c2535442537442532382535436d6174686266253742572537442535452537422535426c2535442537442532432532302535436d6174686266253742622537442535452537422535426c2535442537442532432532302535436d6174686266253742642537442535452537422535426c25354425374425323925323025334425323025354366726163253742312537442537426d25374425323025354373756d5f25374274253344312537442535452537426d2537442532304c25323879253545253742253238742532392537442532432532302535436861742537427925374425354525374225323874253239253744253239"><img src="https://camo.githubusercontent.com/7b9009df0b7ca290c19660274a0f69ddb0d4cb7bb833ed763f6d7318ffbf1175/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c2535452537422535426c2535442537442532382535436d6174686266253742572537442535452537422535426c2535442537442532432532302535436d6174686266253742622537442535452537422535426c2535442537442532432532302535436d6174686266253742642537442535452537422535426c25354425374425323925323025334425323025354366726163253742312537442537426d25374425323025354373756d5f25374274253344312537442535452537426d2537442532304c25323879253545253742253238742532392537442532432532302535436861742537427925374425354525374225323874253239253744253239" alt="L^{[l]}(\mathbf{W}^{[l]}, \mathbf{b}^{[l]}, \mathbf{d}^{[l]}) = \frac{1}{m} \sum_{t=1}^{m} L(y^{(t)}, \hat{y}^{(t)})" data-canonical-src="https://latex.csdn.net/eq?L%5E%7B%5Bl%5D%7D%28%5Cmathbf%7BW%7D%5E%7B%5Bl%5D%7D%2C%20%5Cmathbf%7Bb%7D%5E%7B%5Bl%5D%7D%2C%20%5Cmathbf%7Bd%7D%5E%7B%5Bl%5D%7D%29%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%20%5Csum_%7Bt%3D1%7D%5E%7Bm%7D%20L%28y%5E%7B%28t%29%7D%2C%20%5Chat%7By%7D%5E%7B%28t%29%7D%29" style="max-width: 100%;"></a></p>
<p>其中<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/44c1d41971475de53e9d4815f3c83a429f115245a195ca22cdfccf7270a15245/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d6174686266253742572537442535452537422535426c253544253744"><img src="https://camo.githubusercontent.com/44c1d41971475de53e9d4815f3c83a429f115245a195ca22cdfccf7270a15245/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d6174686266253742572537442535452537422535426c253544253744" alt="\mathbf{W}^{[l]}" data-canonical-src="https://latex.csdn.net/eq?%5Cmathbf%7BW%7D%5E%7B%5Bl%5D%7D" style="max-width: 100%;"></a>和<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/622707aa41c1b2b826027a90694b4f5c69c46411d553aa4495c7fb90e1815a06/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d6174686266253742622537442535452537422535426c253544253744"><img src="https://camo.githubusercontent.com/622707aa41c1b2b826027a90694b4f5c69c46411d553aa4495c7fb90e1815a06/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436d6174686266253742622537442535452537422535426c253544253744" alt="\mathbf{b}^{[l]}" data-canonical-src="https://latex.csdn.net/eq?%5Cmathbf%7Bb%7D%5E%7B%5Bl%5D%7D" style="max-width: 100%;"></a> 是第<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6a4395d82439bacc026584dd128362e93193053929c7633ec045f437c7a85221/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f6c"><img src="https://camo.githubusercontent.com/6a4395d82439bacc026584dd128362e93193053929c7633ec045f437c7a85221/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f6c" alt="l" data-canonical-src="https://latex.csdn.net/eq?l" style="max-width: 100%;"></a>层的权重和偏置，<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9a3a3054f8009491394b6033251f0fa2ec47ec37dcc510c973ce4798364ddadd/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c25323879253545253742253238742532392537442532432532302535436861742537427925374425354525374225323874253239253744253239"><img src="https://camo.githubusercontent.com/9a3a3054f8009491394b6033251f0fa2ec47ec37dcc510c973ce4798364ddadd/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f4c25323879253545253742253238742532392537442532432532302535436861742537427925374425354525374225323874253239253744253239" alt="L(y^{(t)}, \hat{y}^{(t)})" data-canonical-src="https://latex.csdn.net/eq?L%28y%5E%7B%28t%29%7D%2C%20%5Chat%7By%7D%5E%7B%28t%29%7D%29" style="max-width: 100%;"></a>是损失函数，<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a0ca3425a7f06ed6a6f338a63e114c056b34e1193429cd764dc2f264e3c50322/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436861742537427925374425354525374225323874253239253744"><img src="https://camo.githubusercontent.com/a0ca3425a7f06ed6a6f338a63e114c056b34e1193429cd764dc2f264e3c50322/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f2535436861742537427925374425354525374225323874253239253744" alt="\hat{y}^{(t)}" data-canonical-src="https://latex.csdn.net/eq?%5Chat%7By%7D%5E%7B%28t%29%7D" style="max-width: 100%;"></a>是网络的输出，<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f575e01e9a53a209626816a63cdba31bc0ae5c8b41a44efe83d2c91392eaf7e1/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f6d"><img src="https://camo.githubusercontent.com/f575e01e9a53a209626816a63cdba31bc0ae5c8b41a44efe83d2c91392eaf7e1/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f6d" alt="m" data-canonical-src="https://latex.csdn.net/eq?m" style="max-width: 100%;"></a>是训练样本数量。</p>
<p>在测试期间，没有随机失活，因此需要通过缩放来调整每个隐藏层的输出。具体地，我们将每个神经元的输出值<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1d313bcc385797838e3a422a7c77cc2382c1da408b1ff5714ac9479a4dd4669b/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f612535452537422535426c253544253744"><img src="https://camo.githubusercontent.com/1d313bcc385797838e3a422a7c77cc2382c1da408b1ff5714ac9479a4dd4669b/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f612535452537422535426c253544253744" alt="a^{[l]}" data-canonical-src="https://latex.csdn.net/eq?a%5E%7B%5Bl%5D%7D" style="max-width: 100%;"></a>乘以保留概率 <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1060da33ac9dc5b118859c9f8e1142b0265e32eca3d3fc0335befb0202a6d790/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f70"><img src="https://camo.githubusercontent.com/1060da33ac9dc5b118859c9f8e1142b0265e32eca3d3fc0335befb0202a6d790/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f70" alt="p" data-canonical-src="https://latex.csdn.net/eq?p" style="max-width: 100%;"></a>并除以<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1060da33ac9dc5b118859c9f8e1142b0265e32eca3d3fc0335befb0202a6d790/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f70"><img src="https://camo.githubusercontent.com/1060da33ac9dc5b118859c9f8e1142b0265e32eca3d3fc0335befb0202a6d790/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f70" alt="p" data-canonical-src="https://latex.csdn.net/eq?p" style="max-width: 100%;"></a> ：</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9136a2ba3aa5cf0628b0d8c5216c72ae405f2f9f564cb8bd01a80c9df74b2643/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f25354374696c6465253742612537442535452537422535426c25354425374425323025334425323025354366726163253742612535452537422535426c25354425374425374425374270253744"><img src="https://camo.githubusercontent.com/9136a2ba3aa5cf0628b0d8c5216c72ae405f2f9f564cb8bd01a80c9df74b2643/68747470733a2f2f6c617465782e6373646e2e6e65742f65713f25354374696c6465253742612537442535452537422535426c25354425374425323025334425323025354366726163253742612535452537422535426c25354425374425374425374270253744" alt="\tilde{a}^{[l]} = \frac{a^{[l]}}{p}" data-canonical-src="https://latex.csdn.net/eq?%5Ctilde%7Ba%7D%5E%7B%5Bl%5D%7D%20%3D%20%5Cfrac%7Ba%5E%7B%5Bl%5D%7D%7D%7Bp%7D" style="max-width: 100%;"></a></p>
<p>通过这种方式，可以在测试期间保持期望输出不变，从而保持一致性。</p>
<p>在实践中，Dropout 的目标是将模型的期望输出与训练和测试期间的实际输出保持一致，从而减少过拟合并提高模型的泛化能力。</p>
<p><strong>代码实现Dropout应用:</strong></p>
<p>基于 PyTorch 框架，并使用 FashionMNIST 数据集来演示如何构建一个卷积神经网络（CNN）并应用 Dropout。在此示例中，我们将加载 FashionMNIST 数据集，创建一个包含 Dropout 层的简单 CNN 模型，并在训练过程中观察 Dropout 对模型性能的影响。</p>
<pre class="notranslate"><code class="notranslate">import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt

# 定义数据转换
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# 加载 FashionMNIST 数据集
trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)

testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)

classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')

# 定义卷积神经网络模型
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)
        self.dropout = nn.Dropout(0.5)  # 添加 Dropout 层

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 64 * 7 * 7)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)  # 在全连接层添加 Dropout
        x = self.fc2(x)
        return x

# 实例化模型和损失函数、优化器
net = Net()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

# 训练模型
for epoch in range(5):  # 在 FashionMNIST 上训练 5 个 epoch
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        if i % 2000 == 1999:
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')

# 在测试集上评估模型
correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the 10000 test images: %d %%' % (
    100 * correct / total))

</code></pre>
<p>在模型中添加了一个 Dropout 层，其丢弃概率为 0.5。然后我们使用 SGD 优化器和交叉熵损失函数来训练模型。最后，我们在测试集上评估了模型的性能。</p>
<h2>总结：</h2>
<p>今天我们学习了<strong>正则化相关概念</strong>、常见神经网络中正则化数学公式及其作用。重点讲解正则化旨在防止模型过拟合，提高模型的泛化能力。常见的正则化方法包括L1和L2正则化，它们通过向损失函数添加正则项来限制模型参数的大小。另外，Dropout技术在训练过程中随机地关闭神经元，以减少神经网络的复杂性和过拟合风险。此外，数据增强也是一种有效的正则化方法，通过对训练数据进行微小的变换来增加数据的多样性，从而帮助模型更好地泛化到新的数据。这些正则化技术通常结合使用以提高模型的性能和鲁棒性。</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b66607b465204f2c1efa3fb9aa33cf030ea49e1d74ea2b3feb1139ecd5179b69/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f626c6f675f6d6967726174652f33303531616336373439386230386235613039643164303837613838616134382e706e67"><img src="https://camo.githubusercontent.com/b66607b465204f2c1efa3fb9aa33cf030ea49e1d74ea2b3feb1139ecd5179b69/68747470733a2f2f692d626c6f672e6373646e696d672e636e2f626c6f675f6d6967726174652f33303531616336373439386230386235613039643164303837613838616134382e706e67" alt="" data-canonical-src="https://i-blog.csdnimg.cn/blog_migrate/3051ac67498b08b5a09d1d087a88aa48.png" style="max-width: 100%;"></a></p>
<p>本文转自 <a href="https://blog.csdn.net/a910247/article/details/137604232?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522172309920716800184197032%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=172309920716800184197032&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-137604232-null-null.142%5Ev100%5Epc_search_result_base9&amp;utm_term=%E6%AD%A3%E5%88%99%E5%8C%96&amp;spm=1018.2226.3001.4187" rel="nofollow">程序小勇的CSDN文章</a>，如有侵权，请联系删除。</p></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright © <span id="copyrightYear"></span> <a href="https://purcolin.github.io">Blog Title</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if(""!=""){
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","purcolin/purcolin.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}
</script>


</html>
