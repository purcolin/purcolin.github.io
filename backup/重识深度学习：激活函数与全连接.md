# 激活函数
## 1. ReLU
```
torch.nn.ReLU(inplace=False)
```
$\text{ReLU}(x) = (x)^+ = \max(0, x)$
![alt text](https://pytorch.org/docs/stable/_images/ReLU.png)
- inplace (bool) – can optionally do the operation in-place. Default: False

## 2. SoftMax
```
torch.nn.Softmax(dim=None)
```
$\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}$

使某个维度下的所有值缩放到（0，1）且总和为1.
## 3. Tanh
```
torch.nn.Tanh(*args, **kwargs)
```
${Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)}$
![tanh](https://pytorch.org/docs/stable/_images/Tanh.png)
## 4. Sigmoid
```
torch.nn.Sigmoid(*args, **kwargs)
```
${Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}$
![Sigmoid](https://pytorch.org/docs/stable/_images/Sigmoid.png)

# 全连接
```
torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)
```

- in_features (int) – size of each input sample

- out_features (int) – size of each output sample

- bias (bool) – If set to False, the layer will not learn an additive bias. Default: True

### 输入输出

- $Input Shape = (*,H_{in})$ 其中 $H_{in}$ = in_features 
- $Output Shape = (*,H_{out})$ 其中 $H_{out}$ = out_features 

